//! üß† Context Engine - Advanced Context Management System
//! 
//! Core Context Engine with dynamic memory, optimization, and learning capabilities

use anyhow::{Result, anyhow};
use std::sync::Arc;
use std::collections::HashMap;
use uuid::Uuid;
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use serde_json::Value;
use tokio::sync::RwLock;
use tracing::{info, debug, warn, error, instrument};

use crate::{
    config::Config,
    memory_store::{MemoryStore, MemoryEntry, MemoryType},
    embedding_service::{EmbeddingService, EmbeddingVector},
    feedback_loop::{FeedbackLoop, FeedbackData},
    context_optimizer::{ContextOptimizer, OptimizedContext},
    knowledge_graph::{KnowledgeGraph, Entity},
    pattern_recognition::{PatternRecognizer, Pattern},
    semantic_search::{SemanticSearchEngine, SearchQuery, SearchResult},
    metrics::{ContextMetrics, PerformanceMetrics},
};

/// üéØ Stan Context Engine
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ContextState {
    /// Engine inicjalizuje siƒô
    Initializing,
    /// Engine jest aktywny i gotowy
    Active,
    /// Engine optymalizuje kontekst
    Optimizing,
    /// Engine uczy siƒô z feedback
    Learning,
    /// Engine jest w trybie maintenance
    Maintenance,
    /// Engine jest wy≈ÇƒÖczony
    Shutdown,
}

/// ‚ùå B≈Çƒôdy Context Engine
#[derive(Debug, thiserror::Error)]
pub enum ContextError {
    #[error("Memory store error: {0}")]
    MemoryStore(String),
    
    #[error("Embedding service error: {0}")]
    EmbeddingService(String),
    
    #[error("Context optimization error: {0}")]
    ContextOptimization(String),
    
    #[error("Knowledge graph error: {0}")]
    KnowledgeGraph(String),
    
    #[error("Pattern recognition error: {0}")]
    PatternRecognition(String),
    
    #[error("Semantic search error: {0}")]
    SemanticSearch(String),
    
    #[error("Feedback loop error: {0}")]
    FeedbackLoop(String),
    
    #[error("Configuration error: {0}")]
    Configuration(String),
    
    #[error("Context not found: {0}")]
    ContextNotFound(Uuid),
    
    #[error("Invalid context quality: {0}")]
    InvalidContextQuality(f64),
}

/// üìù Context Request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContextRequest {
    /// ID ≈ºƒÖdania
    pub id: Uuid,
    /// Zapytanie/prompt
    pub query: String,
    /// Typ kontekstu
    pub context_type: ContextType,
    /// Maksymalny rozmiar kontekstu
    pub max_context_size: Option<usize>,
    /// Wymagana jako≈õƒá kontekstu
    pub required_quality: Option<f64>,
    /// Metadane ≈ºƒÖdania
    pub metadata: HashMap<String, Value>,
    /// Czas utworzenia
    pub created_at: DateTime<Utc>,
}

/// üéØ Typ kontekstu
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ContextType {
    /// Kontekst dla analizy token√≥w
    TokenAnalysis,
    /// Kontekst dla decyzji tradingowych
    TradingDecision,
    /// Kontekst dla oceny ryzyka
    RiskAssessment,
    /// Kontekst dla analizy sentymentu
    SentimentAnalysis,
    /// Kontekst og√≥lny
    General,
}

/// üìä Context Response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContextResponse {
    /// ID ≈ºƒÖdania
    pub request_id: Uuid,
    /// Zoptymalizowany kontekst
    pub optimized_context: OptimizedContext,
    /// Jako≈õƒá kontekstu
    pub quality_score: f64,
    /// Relevance score
    pub relevance_score: f64,
    /// U≈ºyte wzorce
    pub patterns_used: Vec<Pattern>,
    /// ≈πr√≥d≈Ça kontekstu
    pub sources: Vec<ContextSource>,
    /// Czas przetwarzania
    pub processing_time_ms: u64,
    /// Metadane odpowiedzi
    pub metadata: HashMap<String, Value>,
}

/// üìö ≈πr√≥d≈Ço kontekstu
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContextSource {
    /// Typ ≈∫r√≥d≈Ça
    pub source_type: SourceType,
    /// ID ≈∫r√≥d≈Ça
    pub source_id: String,
    /// Waga w kontek≈õcie
    pub weight: f64,
    /// Confidence score
    pub confidence: f64,
}

/// üîç Typ ≈∫r√≥d≈Ça
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SourceType {
    /// Pamiƒôƒá kr√≥tkoterminowa
    ShortTermMemory,
    /// Pamiƒôƒá d≈Çugoterminowa
    LongTermMemory,
    /// Knowledge graph
    KnowledgeGraph,
    /// Wzorce historyczne
    HistoricalPatterns,
    /// Dane zewnƒôtrzne
    ExternalData,
}

/// üß† G≈Ç√≥wny Context Engine
pub struct ContextEngine {
    /// Konfiguracja
    config: Arc<Config>,
    
    /// Magazyn pamiƒôci
    memory_store: Arc<MemoryStore>,
    
    /// Serwis embeddings
    embedding_service: Arc<EmbeddingService>,
    
    /// Pƒôtla feedback
    feedback_loop: Arc<RwLock<FeedbackLoop>>,
    
    /// Optymalizator kontekstu
    context_optimizer: Arc<ContextOptimizer>,
    
    /// Knowledge graph
    knowledge_graph: Arc<RwLock<KnowledgeGraph>>,
    
    /// Rozpoznawanie wzorc√≥w
    pattern_recognizer: Arc<RwLock<PatternRecognizer>>,
    
    /// Semantic search engine
    semantic_search: Arc<SemanticSearchEngine>,
    
    /// Metryki
    metrics: Arc<RwLock<ContextMetrics>>,
    
    /// Aktualny stan
    state: Arc<RwLock<ContextState>>,
    
    /// Cache kontekst√≥w
    context_cache: Arc<RwLock<HashMap<String, ContextResponse>>>,
}

impl ContextEngine {
    /// Tworzy nowy Context Engine
    #[instrument(skip(config))]
    pub async fn new(config: Arc<Config>) -> Result<Self, ContextError> {
        info!("üß† Initializing Context Engine (CEM)...");
        
        // Walidacja konfiguracji
        config.validate()
            .map_err(|e| ContextError::Configuration(e.to_string()))?;
        
        // Inicjalizacja komponent√≥w
        let memory_store = Arc::new(
            MemoryStore::new(config.clone()).await
                .map_err(|e| ContextError::MemoryStore(e.to_string()))?
        );
        
        let embedding_service = Arc::new(
            EmbeddingService::new(config.clone()).await
                .map_err(|e| ContextError::EmbeddingService(e.to_string()))?
        );
        
        let feedback_loop = Arc::new(RwLock::new(
            FeedbackLoop::new(config.clone()).await
                .map_err(|e| ContextError::FeedbackLoop(e.to_string()))?
        ));
        
        let context_optimizer = Arc::new(
            ContextOptimizer::new(config.clone()).await
                .map_err(|e| ContextError::ContextOptimization(e.to_string()))?
        );
        
        let knowledge_graph = Arc::new(RwLock::new(
            KnowledgeGraph::new(config.clone()).await
                .map_err(|e| ContextError::KnowledgeGraph(e.to_string()))?
        ));
        
        let pattern_recognizer = Arc::new(RwLock::new(
            PatternRecognizer::new(config.clone()).await
                .map_err(|e| ContextError::PatternRecognition(e.to_string()))?
        ));
        
        let semantic_search = Arc::new(
            SemanticSearchEngine::new(config.clone(), memory_store.clone()).await
                .map_err(|e| ContextError::SemanticSearch(e.to_string()))?
        );
        
        let metrics = Arc::new(RwLock::new(ContextMetrics::new()));
        let state = Arc::new(RwLock::new(ContextState::Initializing));
        let context_cache = Arc::new(RwLock::new(HashMap::new()));
        
        info!("‚úÖ Context Engine initialized successfully");
        
        Ok(ContextEngine {
            config,
            memory_store,
            embedding_service,
            feedback_loop,
            context_optimizer,
            knowledge_graph,
            pattern_recognizer,
            semantic_search,
            metrics,
            state,
            context_cache,
        })
    }
    
    /// Uruchamia Context Engine
    #[instrument(skip(self))]
    pub async fn start(&self) -> Result<(), ContextError> {
        info!("üöÄ Starting Context Engine...");
        
        // Ustawienie stanu na aktywny
        {
            let mut state = self.state.write().await;
            *state = ContextState::Active;
        }
        
        // Uruchomienie pƒôtli feedback
        self.start_feedback_loop().await?;
        
        // Uruchomienie optymalizacji kontekstu
        self.start_optimization_loop().await?;
        
        // Uruchomienie pattern recognition
        self.start_pattern_recognition_loop().await?;
        
        info!("‚úÖ Context Engine started successfully");
        Ok(())
    }
    
    /// Przetwarza ≈ºƒÖdanie kontekstu
    #[instrument(skip(self, request))]
    pub async fn process_context_request(
        &self,
        request: ContextRequest,
    ) -> Result<ContextResponse, ContextError> {
        debug!("üß† Processing context request: {}", request.id);
        
        let start_time = std::time::Instant::now();
        
        // Sprawd≈∫ cache
        let cache_key = self.generate_cache_key(&request);
        if let Some(cached_response) = self.get_cached_response(&cache_key).await {
            debug!("üìã Returning cached response for: {}", request.id);
            return Ok(cached_response);
        }
        
        // Zmie≈Ñ stan na optymalizowanie
        {
            let mut state = self.state.write().await;
            *state = ContextState::Optimizing;
        }
        
        // Generuj embedding dla zapytania
        let query_embedding = self.embedding_service
            .generate_embedding(&request.query).await
            .map_err(|e| ContextError::EmbeddingService(e.to_string()))?;
        
        // Wyszukaj relevantne konteksty
        let search_results = self.search_relevant_contexts(&request, &query_embedding).await?;
        
        // Rozpoznaj wzorce
        let patterns = self.recognize_patterns(&request, &search_results).await?;
        
        // Optymalizuj kontekst
        let optimized_context = self.optimize_context(&request, &search_results, &patterns).await?;
        
        // Oblicz jako≈õƒá i relevance
        let quality_score = self.calculate_quality_score(&optimized_context).await?;
        let relevance_score = self.calculate_relevance_score(&request, &optimized_context).await?;
        
        // Utw√≥rz odpowied≈∫
        let response = ContextResponse {
            request_id: request.id,
            optimized_context,
            quality_score,
            relevance_score,
            patterns_used: patterns,
            sources: self.extract_sources(&search_results),
            processing_time_ms: start_time.elapsed().as_millis() as u64,
            metadata: HashMap::new(),
        };
        
        // Zapisz w cache
        self.cache_response(&cache_key, &response).await;
        
        // Aktualizuj metryki
        self.update_metrics(&response).await;
        
        // Zmie≈Ñ stan z powrotem na aktywny
        {
            let mut state = self.state.write().await;
            *state = ContextState::Active;
        }
        
        info!("‚úÖ Context request processed: {} (quality: {:.2}, relevance: {:.2})", 
              request.id, quality_score, relevance_score);
        
        Ok(response)
    }
    
    /// Zapisuje feedback dla kontekstu
    #[instrument(skip(self, feedback))]
    pub async fn record_feedback(&self, feedback: FeedbackData) -> Result<(), ContextError> {
        debug!("üìù Recording feedback for context: {}", feedback.context_id);
        
        // Zmie≈Ñ stan na uczenie
        {
            let mut state = self.state.write().await;
            *state = ContextState::Learning;
        }
        
        // Zapisz feedback
        {
            let mut feedback_loop = self.feedback_loop.write().await;
            feedback_loop.record_feedback(feedback).await
                .map_err(|e| ContextError::FeedbackLoop(e.to_string()))?;
        }
        
        // Zmie≈Ñ stan z powrotem na aktywny
        {
            let mut state = self.state.write().await;
            *state = ContextState::Active;
        }
        
        info!("‚úÖ Feedback recorded successfully");
        Ok(())
    }
    
    /// Pobiera aktualny stan
    pub async fn get_state(&self) -> ContextState {
        let state = self.state.read().await;
        state.clone()
    }
    
    /// Pobiera metryki
    pub async fn get_metrics(&self) -> ContextMetrics {
        let metrics = self.metrics.read().await;
        metrics.clone()
    }
    
    // Metody pomocnicze bƒôdƒÖ dodane w nastƒôpnej iteracji...
    
    /// Generuje klucz cache
    fn generate_cache_key(&self, request: &ContextRequest) -> String {
        format!("{}_{:?}_{}", 
                request.query, 
                request.context_type, 
                request.max_context_size.unwrap_or(0))
    }
    
    /// Pobiera odpowied≈∫ z cache
    async fn get_cached_response(&self, cache_key: &str) -> Option<ContextResponse> {
        let cache = self.context_cache.read().await;
        cache.get(cache_key).cloned()
    }
    
    /// Zapisuje odpowied≈∫ w cache
    async fn cache_response(&self, cache_key: &str, response: &ContextResponse) {
        let mut cache = self.context_cache.write().await;
        cache.insert(cache_key.to_string(), response.clone());
        
        // Ogranicz rozmiar cache
        if cache.len() > 1000 {
            let oldest_key = cache.keys().next().unwrap().clone();
            cache.remove(&oldest_key);
        }
    }
    
    /// Wyszukuje relevantne konteksty
    async fn search_relevant_contexts(
        &self,
        request: &ContextRequest,
        query_embedding: &EmbeddingVector,
    ) -> Result<Vec<SearchResult>, ContextError> {
        let search_query = SearchQuery {
            embedding: query_embedding.clone(),
            limit: 20,
            threshold: self.config.optimization.relevance_threshold,
            context_type: Some(request.context_type.clone()),
        };
        
        self.semantic_search.search(search_query).await
            .map_err(|e| ContextError::SemanticSearch(e.to_string()))
    }
    
    /// Rozpoznaje wzorce
    async fn recognize_patterns(
        &self,
        _request: &ContextRequest,
        _search_results: &[SearchResult],
    ) -> Result<Vec<Pattern>, ContextError> {
        // Implementacja pattern recognition
        Ok(vec![])
    }
    
    /// Optymalizuje kontekst
    async fn optimize_context(
        &self,
        request: &ContextRequest,
        search_results: &[SearchResult],
        patterns: &[Pattern],
    ) -> Result<OptimizedContext, ContextError> {
        self.context_optimizer
            .optimize(request, search_results, patterns).await
            .map_err(|e| ContextError::ContextOptimization(e.to_string()))
    }
    
    /// Oblicza jako≈õƒá kontekstu
    async fn calculate_quality_score(&self, _context: &OptimizedContext) -> Result<f64, ContextError> {
        // Implementacja quality scoring
        Ok(0.8)
    }
    
    /// Oblicza relevance score
    async fn calculate_relevance_score(
        &self,
        _request: &ContextRequest,
        _context: &OptimizedContext,
    ) -> Result<f64, ContextError> {
        // Implementacja relevance scoring
        Ok(0.7)
    }
    
    /// Ekstraktuje ≈∫r√≥d≈Ça
    fn extract_sources(&self, search_results: &[SearchResult]) -> Vec<ContextSource> {
        search_results.iter().map(|result| ContextSource {
            source_type: SourceType::LongTermMemory,
            source_id: result.id.to_string(),
            weight: result.score,
            confidence: result.confidence,
        }).collect()
    }
    
    /// Aktualizuje metryki
    async fn update_metrics(&self, response: &ContextResponse) {
        let mut metrics = self.metrics.write().await;
        metrics.record_context_request(
            response.processing_time_ms,
            response.quality_score,
            response.relevance_score,
        );
    }
    
    /// Uruchamia pƒôtlƒô feedback
    async fn start_feedback_loop(&self) -> Result<(), ContextError> {
        // Implementation bƒôdzie dodana w nastƒôpnej iteracji
        Ok(())
    }
    
    /// Uruchamia pƒôtlƒô optymalizacji
    async fn start_optimization_loop(&self) -> Result<(), ContextError> {
        // Implementation bƒôdzie dodana w nastƒôpnej iteracji
        Ok(())
    }
    
    /// Uruchamia pƒôtlƒô pattern recognition
    async fn start_pattern_recognition_loop(&self) -> Result<(), ContextError> {
        // Implementation bƒôdzie dodana w nastƒôpnej iteracji
        Ok(())
    }
}
